# Docs for the Azure Web Apps Deploy action: https://github.com/azure/functions-action
# More GitHub Actions for Azure: https://github.com/Azure/actions
# More info on Python, GitHub Actions, and Azure Functions: https://aka.ms/python-webapps-actions

name: Evaluate Action and Deploy to Azure Function App

on:
  push:
    branches:
      - main
  workflow_dispatch:    

env:
  AZURE_FUNCTIONAPP_PACKAGE_PATH: '.' # set this to the path to your web app project, defaults to the repository root
  PYTHON_VERSION: '3.12' # set this to the python version to use (supports 3.6, 3.7, 3.8)
  GENAI_EVALS_CONFIG_PATH: ${{ github.workspace }}/evaluate-config.json 
  GENAI_EVALS_DATA_PATH: ${{ github.workspace }}/testdata/genai-testdata-new.jsonl
  AGENT_DATA_DRIVEN_TEST_REPORT_PATH: ${{ github.workspace }}/testdata/Agent_DataDriven_Test_Report.html
  RUN_PYTHON_TESTS: ${{ secrets.RUN_PYTHON_TESTS }}  # set to 'TRUE' to run python tests
  storage_account_url: ${{ secrets.AZURE_STORAGE_ACCOUNT_URL }}
  storage_account_key: ${{ secrets.AZURE_STORAGE_ACCOUNT_KEY }}
  cosmos_db_endpoint: ${{ secrets.COSMOS_DB_ENDPOINT }}
  cosmos_db_key: ${{ secrets.COSMOS_DB_KEY }}
  AZURE_OPENAI_ENDPOINT: ${{ secrets.MODEL_ENDPOINT }}
  AZURE_OPENAI_API_KEY: ${{ secrets.MODEL_API_KEY }}
  AZURE_OPENAI_DEPLOYMENT_NAME: ${{ secrets.MODEL_DEPLOYMENT_NAME }}
  AZURE_OPENAI_API_VERSION: ${{ secrets.MODEL_VERSION }}
  AZURE_OPENAI_MODEL: ${{ secrets.MODEL_DEPLOYMENT_NAME }}
  MODEL_ENDPOINT: ${{ secrets.MODEL_ENDPOINT }}
  MODEL_API_KEY: ${{ secrets.MODEL_API_KEY }}
  MODEL_DEPLOYMENT_NAME: ${{ secrets.MODEL_DEPLOYMENT_NAME }}
  MODEL_VERSION: ${{ secrets.MODEL_VERSION }}
  AZURE_SERVICE_ENDPOINT: ${{ secrets.AZURE_SERVICE_ENDPOINT }}
  PROJECT_ENDPOINT: ${{ secrets.PROJECT_ENDPOINT }}
jobs:
  build:
    runs-on: ubuntu-latest
    permissions:
      contents: read #This is required for actions/checkout    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python version
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Create and start virtual environment
        run: |
          python -m venv venv
          source venv/bin/activate

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt


      # Optional: Add step to run tests here

      - name: Zip artifact for deployment
        run: zip release.zip ./* -r

      - name: Upload artifact for deployment job
        uses: actions/upload-artifact@v4
        with:
          name: python-app
          path: |
            release.zip
            !venv/

  deploy:
    runs-on: ubuntu-latest
    needs: build
    permissions:
      id-token: write #This is required for requesting the JWT
      contents: read #This is required for actions/checkout

    steps:
      - name: Download artifact from build job
        uses: actions/download-artifact@v4
        with:
          name: python-app

      # New step: validate required environment variables (fail early with clear errors)
      # - name: Validate required environment variables
      #   run: |
      #     set -euo pipefail
      #     missing=0
      #     for v in AZURE_OPENAI_ENDPOINT AZURE_OPENAI_API_KEY AZURE_OPENAI_DEPLOYMENT_NAME AZURE_OPENAI_API_VERSION storage_account_url storage_account_key cosmos_db_endpoint cosmos_db_key; do
      #       if [ -z "${!v:-}" ]; then
      #         echo "ERROR: required env var '$v' is not set or empty"
      #         missing=1
      #       fi
      #     done
      #     if [ "$missing" -eq 1 ]; then
      #       echo "One or more required env vars are missing; failing early."
      #       exit 1
      #     fi    

      - name: Unzip artifact for deployment
        run: |
          unzip release.zip
          rm release.zip
      
      - name: Login to Azure
        if: ${{ env.RUN_PYTHON_TESTS == 'TRUE' }}
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZUREAPPSERVICE_CLIENTID_9A0E4792C5E44D14A97D3A37E1CE4D3B }}
          tenant-id: ${{ secrets.AZUREAPPSERVICE_TENANTID_A3BFE828FBA6459E9F6ECAE9B88011F6 }}
          subscription-id: ${{ secrets.AZUREAPPSERVICE_SUBSCRIPTIONID_63FE40BDC05F400CA3E1655311435D8E }}

      
      # - name: Run Evaluation
      #   uses: microsoft/ai-agent-evals@v2-beta
      #   with:
      #     # Replace placeholders with values for your Azure AI Project
      #     azure-ai-project-endpoint: ${{ secrets.PROJECT_ENDPOINT }}
      #     deployment-name: ${{ secrets.MODEL_DEPLOYMENT_NAME }}
      #     agent-ids: ${{ secrets.AGENT_ID }}
      #     data-path: ${{ github.workspace }}/testdata/data.json

      # - name: Write evaluate config 
      #   if: ${{ env.RUN_PYTHON_TESTS == 'TRUE' }}
      #   run: |
      #     cat > ${{ env.GENAI_EVALS_CONFIG_PATH }} <<EOF
      #     {
      #       "data": "${{ env.GENAI_EVALS_DATA_PATH }}",
      #       "evaluators": {
      #         "coherence": "CoherenceEvaluator",
      #         "fluency": "FluencyEvaluator"            
      #       },

      #       "ai_model_configuration": {
      #         "type":"azure_openai",
      #         "azure_endpoint":"${{ secrets.MODEL_ENDPOINT }}",
      #         "azure_deployment":"${{ secrets.MODEL_DEPLOYMENT_NAME }}",
      #         "api_key":"${{ secrets.MODEL_API_KEY }}",
      #         "api_version":"${{ secrets.MODEL_VERSION }}"            
      #       }
      #     }
      #     EOF
 
      # - name: Run AI Evaluation 
      #   if: ${{ env.RUN_PYTHON_TESTS == 'TRUE' }}
      #   id: run-ai-evaluation 
      #   uses: microsoft/genai-evals@main 
      #   with: 
      #     evaluate-configuration: ${{ env.GENAI_EVALS_CONFIG_PATH }}
          # azure_ai_project: |
          #       {
          #         "name": "${{ env.AZURE_OPENAI_ENDPOINT }}",
          #         "description": "Evaluation project for Azure AI",
          #         "tags": {
          #           "owner": "Suganyarani",
          #           "environment": "dev"
          #         }
          #       }

      
      
      - name: Run unit tests (agents)
        id: run_tests
        if: ${{ env.RUN_PYTHON_TESTS == 'TRUE' }}
        run: |
            # If the build job uploaded a venv, activate it and run tests immediately.
            # (Make sure your build job uploaded 'venv/' as an artifact.)
            if [ -d "./venv" ]; then
            echo "Using venv uploaded from build job"
            source venv/bin/activate
            # Optional: upgrade pip or install missing packages if needed
            pip install --upgrade pip
            pip install -r requirements.txt
            python -m unittest discover -s tests -p "*_test.py" -v > unittest.log 2>&1 || true 
            exit 0
            fi
            
      - name: Publish short test summary to Actions tab  
        if: ${{ env.RUN_PYTHON_TESTS == 'TRUE' }}      
        run: |
          echo "## ðŸ§ª Unit Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric   | Count |" >> $GITHUB_STEP_SUMMARY
          echo "|----------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Tests Run | $TESTS_RUN |" >> $GITHUB_STEP_SUMMARY
          echo "| Failures  | $FAILURES |" >> $GITHUB_STEP_SUMMARY
          echo "| Errors    | $ERRORS |" >> $GITHUB_STEP_SUMMARY
          echo "| Skipped   | $SKIPPED |" >> $GITHUB_STEP_SUMMARY
        env:
          TESTS_RUN: ${{ steps.run_tests.outputs.tests_run }}
          FAILURES: ${{ steps.run_tests.outputs.failures }}
          ERRORS: ${{ steps.run_tests.outputs.errors }}
          SKIPPED: ${{ steps.run_tests.outputs.skipped }}
      
      - name: Unit Test Report
        if: ${{ env.RUN_PYTHON_TESTS == 'TRUE' }}
        uses: actions/upload-artifact@v4
        with:
          name: html-report
          path: ${{ env.AGENT_DATA_DRIVEN_TEST_REPORT_PATH }}

      
      # - name: Add link to summary
      #   run: echo "[View Report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id}}"         


      - name: Run evaluation.py script
        id: run-eval
        
        if: ${{ env.RUN_PYTHON_TESTS == 'TRUE' }}
        run: |
          python -m venv venv
          source venv/bin/activate
          pip install --upgrade pip
          pip install -r requirements.txt
          # python evaluation.py ${{ github.workspace }}/testdata/test_data.jsonl #> output.txt
          # echo "result=$(cat output.txt)" >> $GITHUB_OUTPUT
          # echo $GITHUB_OUTPUT
          # Rerun evaluation and capture full stdout/stderr
          python evaluation.py  ${{ github.workspace }}/testdata/test_data.jsonl > eval_output.txt 2>&1 || true

          # Publish to the Actions job summary (visible in the UI after job completes)
          echo "## Evaluation Results" >> "$GITHUB_STEP_SUMMARY"
          echo '```' >> "$GITHUB_STEP_SUMMARY"
          cat eval_output.txt >> "$GITHUB_STEP_SUMMARY"
          echo '```' >> "$GITHUB_STEP_SUMMARY"

          # Expose the evaluation output as a step output (multiline-safe) for downstream jobs
          {
            echo "evaluation_result<<EOF"
            cat eval_output.txt
            echo "EOF"
          } >> "$GITHUB_OUTPUT"   
          


      - name: Upload full logs (optional)
        if: ${{ env.RUN_PYTHON_TESTS == 'TRUE' }}
        uses: actions/upload-artifact@v4
        with:
          name: unittest-logs
          path: unittest.log

    
      - name: 'Deploy to Azure Functions'
        if: ${{ env.RUN_PYTHON_TESTS == 'TRUE' && steps.run_tests.outputs.failures == 0 }}
        uses: Azure/functions-action@v1
        id: deploy-to-function
        with:
          app-name: 'azure-function-cicd'
          slot-name: 'Production'
          package: ${{ env.AZURE_FUNCTIONAPP_PACKAGE_PATH }}      
 

        